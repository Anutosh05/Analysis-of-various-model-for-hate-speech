{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a6d6b5",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using LSTM\n",
    "This notebook demonstrates the creation of a bidirectional LSTM model for text classification.\n",
    "The dataset used here contains preprocessed tweets categorized into three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1c261",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "We'll begin by importing the necessary libraries for data processing, model building, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8e470",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "We will load the preprocessed dataset, which is assumed to be stored in a CSV file named `processed_text.csv`. The dataset contains a `clean_tweet` column with preprocessed text and a `class` column indicating the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c1eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08ab61",
   "metadata": {},
   "source": [
    "## Splitting Input and Target Variables\n",
    "We'll separate the `clean_tweet` column as the input (X) and the `class` column as the target variable (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe21cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_tweet'].values\n",
    "y = df['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884cc39c",
   "metadata": {},
   "source": [
    "## Tokenizing the Text Data\n",
    "Text data will be tokenized into numerical sequences using Keras's `Tokenizer`. We'll also calculate the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ab3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c41794",
   "metadata": {},
   "source": [
    "## Padding Sequences\n",
    "Since sequences have varying lengths, we'll pad them to ensure uniformity. This step is crucial for processing data through the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(sequence) for sequence in X)\n",
    "X = pad_sequences(X, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf9538",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Testing Sets\n",
    "The data will be split into training and testing sets using an 80-20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81801bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa06d79",
   "metadata": {},
   "source": [
    "## Building the Bidirectional LSTM Model\n",
    "The model includes embedding layers, multiple bidirectional LSTMs, and dense layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfd3556",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "The model is compiled with the sparse categorical crossentropy loss function, Adam optimizer, and accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c15c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2da8c9",
   "metadata": {},
   "source": [
    "## Adding Early Stopping Callback\n",
    "Early stopping will prevent overfitting by stopping training when the validation accuracy stops improving for three consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd87f0b",
   "metadata": {},
   "source": [
    "## Model Summary\n",
    "Let's inspect the architecture of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d995bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554ef3a",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "We train the model using a batch size of 64 and validate it on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=64, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d57fb9",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "Finally, we'll evaluate the model on the test data to calculate its accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}