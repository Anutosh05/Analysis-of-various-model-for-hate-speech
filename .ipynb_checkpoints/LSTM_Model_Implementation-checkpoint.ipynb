{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d698784",
   "metadata": {},
   "source": [
    "# LSTM Model Implementation\n",
    "This notebook demonstrates a complete workflow for building and training an LSTM-based model on text data. The steps include preprocessing, model creation, training, evaluation, and performance visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4124826f",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "We start by importing the required libraries for data processing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from save_data import save_data\n",
    "from data_split_ready import split_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7784e",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We load and preprocess the text data, converting it to sequences using a tokenizer and padding to ensure uniform input length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4afb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and split data\n",
    "X_train, X_test, y_train, y_test, tokenizer = split_data('processed_text.csv')\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(sequence) for sequence in X_train)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430bb449",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "We build an LSTM model with embedding layers, dropout, and dense layers for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define LSTM Model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 100, input_length=max_length),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c8713",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "We train the model with early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64, callbacks=[early_stopping])\n",
    "\n",
    "# Save initial performance metrics\n",
    "save_data(model, X_test, y_test, 'LSTM without class weights')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145d766",
   "metadata": {},
   "source": [
    "## Dynamic Class Weights\n",
    "We adjust class weights dynamically based on recall performance to handle class imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227813f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dynamic Class Weights\n",
    "class_weights = {0: 5, 1: 1, 2: 2}\n",
    "epochs = 10\n",
    "patience = 2\n",
    "min_delta = 0.001\n",
    "best_recall = 0\n",
    "patience_counter = 0\n",
    "num_classes = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.fit(X_train, y_train, epochs=1, batch_size=64, class_weight=class_weights)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "    recall = recall_score(y_test, y_pred_classes, average=None, zero_division=0)\n",
    "    mean_recall = np.mean(recall)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Recall: {recall}, Mean Recall: {mean_recall}')\n",
    "\n",
    "    if mean_recall > best_recall + min_delta:\n",
    "        best_recall = mean_recall\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered. No significant improvement in recall.\")\n",
    "        break\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        if recall[i] < 0.5:\n",
    "            class_weights[i] *= 1.2\n",
    "        elif recall[i] < 0.7:\n",
    "            class_weights[i] *= 1.1\n",
    "        else:\n",
    "            class_weights[i] *= 0.9\n",
    "\n",
    "    total_weight = sum(class_weights.values())\n",
    "    class_weights = {k: v / total_weight for k, v in class_weights.items()}\n",
    "\n",
    "    print(f'Updated Class Weights: {class_weights}')\n",
    "\n",
    "# Save metrics after dynamic class weights\n",
    "save_data(model, X_test, y_test, 'LSTM with class weights')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0229e6c",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "We evaluate the model's performance using precision, recall, and ROC-AUC metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e85ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate Model\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "print(\"\n",
    "Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# ROC Curve\n",
    "y_test_binarized = label_binarize(y_test, classes=range(num_classes))\n",
    "auc_per_class = roc_auc_score(y_test_binarized, y_pred_probs, average=None)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(num_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_probs[:, i])\n",
    "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {auc_per_class[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.title('ROC Curve for Each Class')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}